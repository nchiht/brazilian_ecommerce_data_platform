version: "3.9"

services:

# Pipelines
  etl_pipeline:
    build:
      context: ./etl_pipeline
      dockerfile: ./Dockerfile
    container_name: etl_pipeline
    image: etl_pipeline:latest
    restart: always
#    ports:
#      - "8888:8888"
    volumes:
      - ./etl_pipeline:/opt/dagster/app
      - ./etl_pipeline/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    env_file:
      - env
    networks:
      - de_network
    
  de_psql:
    image: postgres:15
    container_name: de_psql
    volumes:
      - ./data/postgresql:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    env_file:
      - env
    networks:
      - de_network

  de_mysql:
    image: mysql:8.0
    container_name: de_mysql
    volumes:
      - ./data/mysql:/var/lib/mysql
    ports:
      - "3306:3306"
    env_file:
      - env
    networks:
      - de_network

  minio:
    hostname: minio
    image: "minio/minio"
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - ./data/minio:/data
    env_file:
      - env
    networks:
      - de_network

  mc:
    image: minio/mc
    container_name: mc
    hostname: mc
    env_file:
      - env
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' && sleep 1;
      done;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      exit 0; "
    depends_on:
      - minio
    networks:
      - de_network

  # Dagster
  de_dagster:
    build:
      context: ./dagster/
    container_name: de_dagster
    image: de_dagster
  
  de_dagster_dagit:
    image: de_dagster:latest
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3001"
      - -w
      - workspace.yaml
    container_name: de_dagster_dagit
    expose:
      - "3001"
    ports:
      - "3001:3001"
    volumes: # Make docker client accessible, so we can terminate containers from dagit
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    depends_on:
      - etl_pipeline
      - de_psql
    env_file:
      - env
    networks:
      - de_network

  de_dagster_daemon:
    image: de_dagster:latest
    entrypoint:
      - dagster-daemon
      - run
    container_name: de_dagster_daemon
    volumes: # Make docker client accessible, so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    depends_on:
      - etl_pipeline
      - de_psql
    env_file:
      - env
    networks:
      - de_network

  # Spark
  spark-master:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: spark_master
    hostname: spark_master
    env_file:
      - spark_master_env
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - de_network

  spark-worker:
    image: docker.io/bitnami/spark:3.3
    deploy:
      replicas: 2
    env_file:
      - spark_worker_env

    networks:
      - de_network

  spark-notebook:
    build:
      context: ./notebooks
      dockerfile: ./Dockerfile
    container_name: spark_notebook
    hostname: spark_notebook
    user: root
    environment:
      - JUPYTER_ENABLE_LAB="yes"
      - GRANT_SUDO="yes"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./notebooks/spark-defaults.conf:/usr/local/spark/conf/spark-defaults.conf
    ports:
      - "8888:8888"
      - "4040:4040"
    networks:
      - de_network

networks:
  de_network:
    driver: bridge
    name: de_network